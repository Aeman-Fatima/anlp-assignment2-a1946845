# NLP StackOverflow Categorization Project (ANLP Assignment 2)

This project analyzes and categorizes Stack Overflow posts tagged with [nlp].

## Folder Structure
anlp-assignment-template/ â”œâ”€â”€ src/ # Source code files for all components â”‚ â”œâ”€â”€ scraper.py # Collects StackOverflow posts using StackExchange API â”‚ â”œâ”€â”€ preprocess.py # Cleans and prepares raw text data â”‚ â”œâ”€â”€ categorize.py # Performs rule-based categorization â”‚ â”œâ”€â”€ visualize.py # Generates WordCloud and category charts â”‚ â””â”€â”€ topic_model.py # Performs topic modeling using LDA â”‚ â”œâ”€â”€ data/ # Datasets and output CSVs â”‚ â”œâ”€â”€ nlp_stackoverflow_sample.csv # Raw dataset collected (~6,248 posts) â”‚ â”œâ”€â”€ preprocessed.csv # Cleaned text after preprocessing â”‚ â”œâ”€â”€ categorized.csv # Posts with assigned categories â”‚ â””â”€â”€ category_summary.csv # Category-wise post count summary â”‚ â”œâ”€â”€ assets/ # Visual assets (WordClouds, Charts) â”‚ â”œâ”€â”€ wordcloud.png â”‚ â”œâ”€â”€ category_distribution.png â”‚ â””â”€â”€ category_over_time.png â”‚ â”œâ”€â”€ requirements.txt # Python dependencies â”œâ”€â”€ main.py # Runs full processing pipeline end-to-end â””â”€â”€ README.md # Project overview and instructions


## How to Run

```bash
# Activate your environment, then run:
python src/main.py
```

### Requirements
Install dependencies with:

```bash
pip install -r requirements.txt
```

> Note: Ensure you have NLTK stopwords and punkt downloaded:
```python
import nltk
nltk.download('punkt')
nltk.download('stopwords')

# or you can download: 
python -c "import nltk; nltk.download('punkt'); nltk.download('stopwords')"

## Output
- Categorized dataset with â‰¥100 labeled posts
- WordCloud
- Bar graph of categories
- Topic modeling results
- Category timeline

## Dataset
The dataset used was collected via the StackExchange API and includes ~6,248 posts. You can find the dataset and all outputs here:

[data/nlp_stackoverflow_sample.csv]


## ğŸ‘©â€ğŸ’» Author
**Aeman Fatima** â€” [a1946845]
